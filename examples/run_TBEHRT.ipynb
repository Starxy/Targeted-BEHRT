{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/Project/HealthDataInst/T-BEHRT/Targeted-BEHRT')\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_pretrained_bert as Bert\n",
    "\n",
    "from  pytorch_pretrained_bert import optimizer\n",
    "# import sklearn.metrics as skm\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from src.utils import *\n",
    "from src.model import *\n",
    "from src.data import *\n",
    "\n",
    "from torch import optim as toptimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 根据不同的beta类型计算beta值\n",
    "def get_beta(batch_idx, m, beta_type):\n",
    "    if beta_type == \"Blundell\":\n",
    "        beta = 2 ** (m - (batch_idx + 1)) / (2 ** m - 1)\n",
    "    elif beta_type == \"Soenderby\":\n",
    "        beta = min(epoch / (num_epochs // 4), 1)\n",
    "    elif beta_type == \"Standard\": \n",
    "        beta = 1 / m\n",
    "    else:\n",
    "        beta = 0\n",
    "    return beta\n",
    "\n",
    "\n",
    "# 无监督训练函数\n",
    "def trainunsup(e, sched, patienceMetric, MEM=True):\n",
    "    \"\"\"\n",
    "    无监督训练函数,实现论文中提到的两部分MEM(Masked EHR Modeling)训练:\n",
    "    1. 时序变量建模(temporal variable modeling):类似MLM,预测被掩盖的医疗记录\n",
    "    2. 静态变量建模(static variable modeling):使用VAE对静态变量(如性别、地区等)进行建模\n",
    "    \n",
    "    Args:\n",
    "        e: 当前训练轮数\n",
    "        sched: 学习率调度器\n",
    "        patienceMetric: 早停指标\n",
    "        MEM: 是否使用MEM训练,默认为True\n",
    "    Returns:\n",
    "        sched: 更新后的学习率调度器\n",
    "        patienceMetric: 更新后的早停指标\n",
    "    \"\"\"\n",
    "    # 重置数据索引,确保每轮训练使用相同的数据顺序\n",
    "    sampled = datatrain.reset_index(drop=True)\n",
    "    \n",
    "    # 创建数据集对象,处理输入数据\n",
    "    # TBEHRT_data_formation类用于处理和格式化医疗数据:\n",
    "    # - BertVocab['token2idx']: 将医疗代码映射为数字ID的词典\n",
    "    # - sampled: 输入的医疗数据样本\n",
    "    # - code='code': 指定包含医疗代码的列名\n",
    "    # - age='age': 指定包含年龄信息的列名 \n",
    "    # - year='year': 指定包含年份信息的列名\n",
    "    # - static='static': 指定包含静态特征(如性别等)的列名\n",
    "    # - max_len: 时序数据的最大长度,超过会被截断\n",
    "    # - expColumn: 指定解释标签的列名\n",
    "    # - outcomeColumn: 指定结果标签的列名\n",
    "    # - yvocab: 年份到ID的映射词典\n",
    "    # - list2avoid: 需要避免的医疗代码列表,默认为None\n",
    "    # - MEM: 是否使用MEM(Masked EHR Modeling)训练模式\n",
    "    Dset = TBEHRT_data_formation(BertVocab['token2idx'], sampled, code= 'code', \n",
    "                                 age = 'age', year = 'year' , static= 'static' , \n",
    "                                 max_len=global_params['max_len_seq'],expColumn='explabel', outcomeColumn='label',  \n",
    "                                 yvocab=YearVocab['token2idx'], list2avoid=None, MEM=MEM)\n",
    "    \n",
    "    # 创建PyTorch数据加载器DataLoader:\n",
    "    # - dataset=Dset: 使用上面创建的数据集\n",
    "    # - batch_size: 每个批次的样本数,由global_params指定\n",
    "    # - shuffle=True: 随机打乱数据顺序\n",
    "    # - num_workers=3: 使用3个子进程并行加载数据\n",
    "    # - sampler=None: 不使用自定义采样器,使用默认的随机采样\n",
    "    trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=3,\n",
    "                           sampler=None)\n",
    "\n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "    \n",
    "    # 初始化训练过程中的各种计数器和损失值\n",
    "    tr_loss = 0  # 总训练损失\n",
    "    temp_loss = 0  # 临时损失(用于定期打印)\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0  # 样本数和步数计数器\n",
    "    oldloss = 10 ** 10  # 用于比较损失变化的旧损失值\n",
    "    \n",
    "    # 遍历每个batch进行训练\n",
    "    for step, batch in enumerate(trainload):\n",
    "        # 将batch中的每个张量t移动到指定的设备(GPU或CPU)上\n",
    "        # global_params['device']指定了使用的设备\n",
    "        # 使用tuple()而不是list的原因:\n",
    "        # 1. 元组是不可变的,可以防止训练过程中意外修改batch数据\n",
    "        # 2. 元组比列表更节省内存\n",
    "        # 3. PyTorch的DataLoader默认返回元组,保持数据类型一致性\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "\n",
    "        # 解包batch数据,包括时序数据和静态数据\n",
    "        age_ids, input_ids, input_idsMLM, posi_ids, segment_ids, year_ids, attMask, masked_label, outcome_label, treatment_label, vaelabel = batch\n",
    "\n",
    "        # 前向传播,获取各种损失和预测结果\n",
    "        # masked_lm_loss: 时序变量MEM损失\n",
    "        # vaelosspure: 静态变量MEM(VAE)损失\n",
    "        masked_lm_loss, lossT, pred, label, treatOut, treatLabel, out, outLabel, treatindex, targreg, vaelosspure = model(\n",
    "            input_idsMLM,\n",
    "            age_ids,\n",
    "            segment_ids,\n",
    "            posi_ids,\n",
    "            year_ids,\n",
    "            attention_mask=attMask,\n",
    "            masked_lm_labels=masked_label,\n",
    "            outcomeT=outcome_label,\n",
    "            treatmentCLabel=treatment_label,\n",
    "            fullEval=False,\n",
    "            vaelabel=vaelabel)\n",
    "            \n",
    "        # 获取VAE损失(静态变量MEM损失)\n",
    "        vaeloss = vaelosspure['loss']\n",
    "\n",
    "        # 计算总损失(这里只使用时序变量MEM损失)\n",
    "        totalL = masked_lm_loss\n",
    "        \n",
    "        # 如果使用梯度累积,则对损失进行相应缩放\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            totalL = totalL / global_params['gradient_accumulation_steps']\n",
    "            \n",
    "        # 反向传播计算梯度\n",
    "        totalL.backward()\n",
    "        \n",
    "        # 获取处理后的预测结果\n",
    "        treatFull = treatOut\n",
    "        treatLabelFull = treatLabel\n",
    "        treatLabelFull = treatLabelFull.cpu().detach()\n",
    "\n",
    "        outFull = out\n",
    "        outLabelFull = outLabel\n",
    "        treatindex = treatindex.cpu().detach().numpy()\n",
    "        # 获取treatment=0的样本索引及对应预测\n",
    "        zeroind = np.where(treatindex == 0)\n",
    "        outzero = outFull[0][zeroind]\n",
    "        outzeroLabel = outLabelFull[zeroind]\n",
    "\n",
    "        # 累计损失值\n",
    "        temp_loss += totalL.item()\n",
    "        tr_loss += totalL.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # 每600步打印一次VAE相关损失(静态变量MEM损失)\n",
    "        if step % 600 == 0:\n",
    "            print([(keyvae, valvae) for (keyvae, valvae) in vaelosspure.items() if\n",
    "                   keyvae in ['loss', 'Reconstruction_Loss', 'KLD']])\n",
    "            # 检查损失是否增加,用于学习率调整\n",
    "            if oldloss < vaelosspure['loss']:\n",
    "                patienceMetric = patienceMetric + 1\n",
    "                if patienceMetric >= 10:\n",
    "                    sched.step()\n",
    "                    print(\"LR: \", sched.get_lr())\n",
    "                    patienceMetric = 0\n",
    "            oldloss = vaelosspure['loss']\n",
    "\n",
    "        # 每200步打印训练状态,包括MLM准确率和VAE损失\n",
    "        if step % 200 == 0:\n",
    "            precOut0 = -1\n",
    "            if len(zeroind[0]) > 0:\n",
    "                precOut0, _, _ = OutcomePrecision(outzero, outzeroLabel, False)\n",
    "\n",
    "            print(\n",
    "                \"epoch: {0}| Loss: {1:6.5f}\\t| MLM: {2:6.5f}\\t| TOutP: {3:6.5f}\\t|vaeloss: {4:6.5f}\\t|ExpP: {5:6.5f}\".format(\n",
    "                    e, temp_loss / 200, cal_acc(label, pred), precOut0, vaeloss,\n",
    "                    cal_acc(treatLabelFull, treatFull, False)))\n",
    "            temp_loss = 0\n",
    "\n",
    "        # 每累积指定步数后进行参数更新\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()  # 更新参数\n",
    "            optim.zero_grad()  # 清空梯度\n",
    "\n",
    "    # 清理内存\n",
    "    del sampled, Dset, trainload\n",
    "    return sched, patienceMetric\n",
    "\n",
    "\n",
    "# 多任务训练函数 - 对应论文中的完整目标函数训练(等式6)\n",
    "def train_multi(e, MEM=True):\n",
    "    sampled = datatrain.reset_index(drop=True)\n",
    "\n",
    "    # 创建数据集 - 包含时序和静态变量\n",
    "    Dset =  TBEHRT_data_formation(BertVocab['token2idx'], sampled, code= 'code', \n",
    "                                 age = 'age', year = 'year' , static= 'static' , \n",
    "                                 max_len=global_params['max_len_seq'],expColumn='explabel', outcomeColumn='label',  \n",
    "                                 yvocab=YearVocab['token2idx'], list2avoid=None, MEM=MEM)\n",
    "    \n",
    "    # 创建数据加载器    \n",
    "    trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'], shuffle=True, num_workers=3,\n",
    "                           sampler=None)\n",
    "    \n",
    "    # 设置模型为训练模式\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    temp_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    # 遍历每个batch进行训练\n",
    "    for step, batch in enumerate(trainload):\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "\n",
    "        # 获取输入数据:包含时序数据(input_idsMLM)和静态数据(vaelabel)\n",
    "        age_ids, input_ids, input_idsMLM, posi_ids, segment_ids, year_ids, attMask, masked_label, outcome_label, treatment_label, vaelabel = batch\n",
    "        \n",
    "        # 前向传播 - 计算三部分损失:\n",
    "        # 1. masked_lm_loss: 时序MEM损失(等式4)\n",
    "        # 2. vaelosspure: 静态MEM损失(等式5) \n",
    "        # 3. lossT: 监督学习损失(等式3)\n",
    "        masked_lm_loss, lossT, pred, label, treatOut, treatLabel, out, outLabel, treatindex, targreg, vaelosspure = model(\n",
    "            input_idsMLM,\n",
    "            age_ids,\n",
    "            segment_ids,\n",
    "            posi_ids,\n",
    "            year_ids,\n",
    "            attention_mask=attMask,\n",
    "            masked_lm_labels=masked_label,\n",
    "            outcomeT=outcome_label,\n",
    "            treatmentCLabel=treatment_label,\n",
    "            fullEval=False,\n",
    "            vaelabel=vaelabel)\n",
    "\n",
    "        vaeloss = vaelosspure['loss']\n",
    "        \n",
    "        # 计算总损失 - 对应论文等式(6)的加权组合\n",
    "        # global_params['fac']对应论文中的超参数δ\n",
    "        totalL = 1 * (lossT) + 0 + (global_params['fac'] * masked_lm_loss)\n",
    "        if global_params['gradient_accumulation_steps'] > 1:\n",
    "            totalL = totalL / global_params['gradient_accumulation_steps']\n",
    "            \n",
    "        # 反向传播\n",
    "        totalL.backward()\n",
    "        \n",
    "        treatFull = treatOut\n",
    "        treatLabelFull = treatLabel\n",
    "        treatLabelFull = treatLabelFull.cpu().detach()\n",
    "\n",
    "        outFull = out\n",
    "        outLabelFull = outLabel\n",
    "        treatindex = treatindex.cpu().detach().numpy()\n",
    "        zeroind = np.where(treatindex == 0)\n",
    "        outzero = outFull[0][zeroind]\n",
    "        outzeroLabel = outLabelFull[zeroind]\n",
    "\n",
    "        # 累计损失\n",
    "        temp_loss += totalL.item()\n",
    "        tr_loss += totalL.item()\n",
    "        nb_tr_examples += input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "        # 每200步打印训练状态,包括:\n",
    "        # - MLM准确率(时序MEM)\n",
    "        # - VAE损失(静态MEM) \n",
    "        # - 倾向性预测和事实结果预测的准确率\n",
    "        if step % 200 == 0:\n",
    "            precOut0 = -1\n",
    "\n",
    "            if len(zeroind[0]) > 0:\n",
    "                precOut0, _, _ = OutcomePrecision(outzero, outzeroLabel, False)\n",
    "\n",
    "            print(\n",
    "                \"epoch: {0}| Loss: {1:6.5f}\\t| MLM: {2:6.5f}\\t| TOutP: {3:6.5f}\\t|vaeloss: {4:6.5f}\\t|ExpP: {5:6.5f}\".format(\n",
    "                    e, temp_loss / 200, cal_acc(label, pred), precOut0, vaeloss,\n",
    "                    cal_acc(treatLabelFull, treatFull, False)))\n",
    "\n",
    "            print([(keyvae, valvae) for (keyvae, valvae) in vaelosspure.items() if\n",
    "                   keyvae in ['loss', 'Reconstruction_Loss', 'KLD']])\n",
    "            temp_loss = 0\n",
    "\n",
    "        # 梯度更新\n",
    "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
    "            optim.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "    # 清理内存\n",
    "    del sampled, Dset, trainload\n",
    "\n",
    "\n",
    "# 多次评估函数\n",
    "def evaluation_multi_repeats():\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    t_label = []\n",
    "    t_output = []\n",
    "    count = 0\n",
    "    totalL = 0\n",
    "    \n",
    "    # 遍历测试数据进行评估\n",
    "    for step, batch in enumerate(testload):\n",
    "        model.eval()\n",
    "        count = count + 1\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "\n",
    "        age_ids, input_ids, input_idsMLM, posi_ids, segment_ids, year_ids, attMask, masked_label, outcome_label, treatment_label, vaelabel = batch\n",
    "        \n",
    "        # 不计算梯度\n",
    "        with torch.no_grad():\n",
    "            masked_lm_loss, lossT, pred, label, treatOut, treatLabel, out, outLabel, treatindex, targreg, vaelosspure = model(\n",
    "                input_idsMLM,\n",
    "                age_ids,\n",
    "                segment_ids,\n",
    "                posi_ids,\n",
    "                year_ids,\n",
    "                attention_mask=attMask,\n",
    "                masked_lm_labels=masked_label,\n",
    "                outcomeT=outcome_label,\n",
    "                treatmentCLabel=treatment_label, vaelabel=vaelabel)\n",
    "\n",
    "        # 累计损失\n",
    "        totalL = totalL + lossT.item() + 0 + (global_params['fac'] * masked_lm_loss)\n",
    "        \n",
    "        treatFull = treatOut\n",
    "        treatLabelFull = treatLabel\n",
    "        treatLabelFull = treatLabelFull.detach()\n",
    "        outFull = out\n",
    "        outLabelFull = outLabel\n",
    "        treatindex = treatindex.cpu().detach().numpy()\n",
    "        \n",
    "        # 收集每个处理的预测结果\n",
    "        outPred = []\n",
    "        outexpLab = []\n",
    "        for el in range(global_params['treatments']):\n",
    "            zeroind = np.where(treatindex == el)\n",
    "            outPred.append(outFull[el][zeroind])\n",
    "            outexpLab.append(outLabelFull[zeroind])\n",
    "\n",
    "        # 收集标签和预测\n",
    "        y_label.append(torch.cat(outexpLab))\n",
    "        y.append(torch.cat(outPred))\n",
    "\n",
    "        treatOut = treatFull.cpu()\n",
    "        treatLabel = treatLabelFull.cpu()\n",
    "        \n",
    "        # 每200步打印评估状态\n",
    "        if step % 200 == 0:\n",
    "            print(step, \"tempLoss:\", totalL / count)\n",
    "\n",
    "        t_label.append(treatLabel)\n",
    "        t_output.append(treatOut)\n",
    "\n",
    "    # 合并所有预测结果\n",
    "    y_label = torch.cat(y_label, dim=0)\n",
    "    y = torch.cat(y, dim=0)\n",
    "    t_label = torch.cat(t_label, dim=0)\n",
    "    treatO = torch.cat(t_output, dim=0)\n",
    "\n",
    "    # 计算精确度和ROC AUC\n",
    "    tempprc, output, label = precision_test(y, y_label, False)\n",
    "    treatPRC = cal_acc(t_label, treatO, False)\n",
    "    tempprc2, output2, label2 = roc_auc(y, y_label, False)\n",
    "\n",
    "    print(\"LossEval: \", float(totalL) / float(count))\n",
    "\n",
    "    return tempprc, tempprc2, treatPRC, float(totalL) / float(count)\n",
    "\n",
    "\n",
    "# 完整评估函数\n",
    "def fullEval_4analysis_multi(tr, te, filetest):\n",
    "    if tr:\n",
    "        sampled = datatrain.reset_index(drop=True)\n",
    "\n",
    "    if te:\n",
    "        data = filetest\n",
    "\n",
    "        if tr:\n",
    "            sampled = pd.concat([sampled, data]).reset_index(drop=True)\n",
    "        else:\n",
    "            sampled = data\n",
    "            \n",
    "    # 创建数据集\n",
    "    Fulltset = TBEHRT_data_formation(BertVocab['token2idx'], sampled, code= 'code', \n",
    "                                 age = 'age', year = 'year' , static= 'static' , \n",
    "                                 max_len=global_params['max_len_seq'],expColumn='explabel', outcomeColumn='label',  \n",
    "                                 yvocab=YearVocab['token2idx'], list2avoid=None, MEM=False)\n",
    "    \n",
    "    # 创建数据加载器    \n",
    "    fullDataLoad = DataLoader(dataset=Fulltset, batch_size=int(global_params['batch_size']), shuffle=False,\n",
    "                              num_workers=0)\n",
    "\n",
    "    # 设置模型为评估模式\n",
    "    model.eval()\n",
    "    y = []\n",
    "    y_label = []\n",
    "    t_label = []\n",
    "    t_output = []\n",
    "    count = 0\n",
    "    totalL = 0\n",
    "    eps_array = []\n",
    "\n",
    "    # 初始化预测列表\n",
    "    for yyy in range(model_config['num_treatment']):\n",
    "        y.append([yyy])\n",
    "        y_label.append([yyy])\n",
    "\n",
    "    print(y)\n",
    "    \n",
    "    # 遍历数据进行评估\n",
    "    for step, batch in enumerate(fullDataLoad):\n",
    "        model.eval()\n",
    "\n",
    "        count = count + 1\n",
    "        batch = tuple(t.to(global_params['device']) for t in batch)\n",
    "\n",
    "        age_ids, input_ids, input_idsMLM, posi_ids, segment_ids, year_ids, attMask, masked_label, outcome_label, treatment_label, vaelabel = batch\n",
    "\n",
    "        # 不计算梯度\n",
    "        with torch.no_grad():\n",
    "            masked_lm_loss, lossT, pred, label, treatOut, treatLabel, out, outLabel, treatindex, targreg, vaeloss = model(\n",
    "                input_idsMLM,\n",
    "                age_ids,\n",
    "                segment_ids,\n",
    "                posi_ids,\n",
    "                year_ids,\n",
    "                attention_mask=attMask,\n",
    "                masked_lm_labels=masked_label,\n",
    "                outcomeT=outcome_label,\n",
    "                treatmentCLabel=treatment_label, fullEval=True, vaelabel=vaelabel)\n",
    "\n",
    "        outFull = out\n",
    "        outLabelFull = outLabel\n",
    "\n",
    "        # 收集每个处理的预测结果\n",
    "        for el in range(global_params['treatments']):\n",
    "            y[el].append(outFull[el].cpu())\n",
    "            y_label[el].append(outLabelFull.cpu())\n",
    "\n",
    "        totalL = totalL + (1 * (lossT)).item()\n",
    "\n",
    "        # 每200步打印评估状态\n",
    "        if step % 200 == 0:\n",
    "            print(step, \"tempLoss:\", totalL / count)\n",
    "\n",
    "        t_label.append(treatLabel)\n",
    "        t_output.append(treatOut)\n",
    "\n",
    "    # 合并预测结果\n",
    "    for idd, elem in enumerate(y):\n",
    "        elem = torch.cat(elem[1:], dim=0)\n",
    "        y[idd] = elem\n",
    "    for idd, elem in enumerate(y_label):\n",
    "        elem = torch.cat(elem[1:], dim=0)\n",
    "        y_label[idd] = elem\n",
    "\n",
    "    t_label = torch.cat(t_label, dim=0)\n",
    "    treatO = torch.cat(t_output, dim=0)\n",
    "    treatPRC = cal_acc(t_label, treatO)\n",
    "\n",
    "    print(\"LossEval: \", float(totalL) / float(count), \"prec treat:\", treatPRC)\n",
    "    return y, y_label, t_label, treatO, treatPRC, eps_array\n",
    "\n",
    "\n",
    "# 转换预测结果的格式\n",
    "def fullCONV(y, y_label, t_label, treatO):\n",
    "    # 转换多热编码\n",
    "    def convert_multihot(label, pred):\n",
    "        label = label.cpu().numpy()\n",
    "        truepred = pred.detach().cpu().numpy()\n",
    "        truelabel = label\n",
    "        newpred = []\n",
    "        for i, x in enumerate(truelabel):\n",
    "            temppred = []\n",
    "            temppred.append(truepred[i][0])\n",
    "            temppred.append(truepred[i][x[0]])\n",
    "            newpred.append(temppred)\n",
    "        return truelabel, np.array(truepred)\n",
    "\n",
    "    # 转换二进制标签\n",
    "    def convert_bin(logits, label, treatmentlabel2):\n",
    "        output = logits\n",
    "        label, output = label.cpu().numpy(), output.detach().cpu().numpy()\n",
    "        label = label[treatmentlabel2[0]]\n",
    "        return label, output\n",
    "\n",
    "    # 转换处理标签\n",
    "    treatmentlabel2, treatment2 = convert_multihot(t_label, treatO)\n",
    "    \n",
    "    # 重塑预测结果\n",
    "    y = torch.cat(y, dim=0).view(global_params['treatments'], -1)\n",
    "    y = y.transpose(1, 0)\n",
    "    y_label = torch.cat(y_label, dim=0).view(global_params['treatments'], -1)\n",
    "    y_label = y_label.transpose(1, 0)\n",
    "    \n",
    "    # 转换预测结果格式\n",
    "    y2 = []\n",
    "    y2label = []\n",
    "    for i, elem in enumerate(y):\n",
    "        j, k = convert_bin(elem, y_label[i], treatmentlabel2[i])\n",
    "        y2.append(k)\n",
    "        y2label.append(j)\n",
    "    y2 = np.array(y2)\n",
    "    y2label = np.array(y2label)\n",
    "    y2label = np.expand_dims(y2label, -1)\n",
    "\n",
    "    return y2, y2label, treatmentlabel2, treatment2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "file_config = {\n",
    "       'data':  'test.parquet',\n",
    "}\n",
    "optim_config = {\n",
    "    'lr': 1e-4,\n",
    "    'warmup_proportion': 0.1\n",
    "}\n",
    "\n",
    "\n",
    "BertVocab = {}\n",
    "token2idx = {'MASK': 4,\n",
    "  'CLS': 3,\n",
    "  'SEP': 2,\n",
    "  'UNK': 1,\n",
    "  'PAD': 0,\n",
    "            'disease1':5,\n",
    "             'disease2':6,\n",
    "             'disease3':7,\n",
    "             'disease4':8,\n",
    "             'disease5':9,\n",
    "             'disease6':10,\n",
    "             'medication1':11,\n",
    "             'medication2':12,\n",
    "             'medication3':13,\n",
    "             'medication4':14,\n",
    "             'medication5':15,\n",
    "             'medication6':16,\n",
    "            }\n",
    "idx2token = {}\n",
    "for x in token2idx:\n",
    "    idx2token[token2idx[x]]=x\n",
    "BertVocab['token2idx']= token2idx\n",
    "BertVocab['idx2token']= idx2token\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "YearVocab = {'token2idx': {'PAD': 0,\n",
    "  '1987': 1,\n",
    "  '1988': 2,\n",
    "  '1989': 3,\n",
    "  '1990': 4,\n",
    "  '1991': 5,\n",
    "  '1992': 6,\n",
    "  '1993': 7,\n",
    "  '1994': 8,\n",
    "  '1995': 9,\n",
    "  '1996': 10,\n",
    "  '1997': 11,\n",
    "  '1998': 12,\n",
    "  '1999': 13,\n",
    "  '2000': 14,\n",
    "  '2001': 15,\n",
    "  '2002': 16,\n",
    "  '2003': 17,\n",
    "  '2004': 18,\n",
    "  '2005': 19,\n",
    "  '2006': 20,\n",
    "  '2007': 21,\n",
    "  '2008': 22,\n",
    "  '2009': 23,\n",
    "  '2010': 24,\n",
    "  '2011': 25,\n",
    "  '2012': 26,\n",
    "  '2013': 27,\n",
    "  '2014': 28,\n",
    "  '2015': 29,\n",
    "  'UNK': 30},\n",
    " 'idx2token': {0: 'PAD',\n",
    "  1: '1987',\n",
    "  2: '1988',\n",
    "  3: '1989',\n",
    "  4: '1990',\n",
    "  5: '1991',\n",
    "  6: '1992',\n",
    "  7: '1993',\n",
    "  8: '1994',\n",
    "  9: '1995',\n",
    "  10: '1996',\n",
    "  11: '1997',\n",
    "  12: '1998',\n",
    "  13: '1999',\n",
    "  14: '2000',\n",
    "  15: '2001',\n",
    "  16: '2002',\n",
    "  17: '2003',\n",
    "  18: '2004',\n",
    "  19: '2005',\n",
    "  20: '2006',\n",
    "  21: '2007',\n",
    "  22: '2008',\n",
    "  23: '2009',\n",
    "  24: '2010',\n",
    "  25: '2011',\n",
    "  26: '2012',\n",
    "  27: '2013',\n",
    "  28: '2014',\n",
    "  29: '2015',\n",
    "  30: 'UNK'}}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "global_params = {\n",
    "    'batch_size': 128,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 3,\n",
    "    'device': 'cuda:0',\n",
    "    'output_dir': \"save_models\",\n",
    "    'save_model': True,\n",
    "    'max_len_seq': 250,\n",
    "    'max_age': 110,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'fac': 0.1,\n",
    "    'diseaseI': 1,\n",
    "    'treatments': 2\n",
    "}\n",
    "\n",
    "ageVocab, _ = age_vocab(max_age=global_params['max_age'], year=global_params['age_year'],\n",
    "                        symbol=global_params['age_symbol'])\n",
    "\n",
    "model_config = {\n",
    "    'vocab_size': len(BertVocab['token2idx'].keys()),  # number of disease + symbols for word embedding\n",
    "    'hidden_size': 150,  # word embedding and seg embedding hidden size\n",
    "    'seg_vocab_size': 2,  # number of vocab for seg embedding\n",
    "    'age_vocab_size': len(ageVocab.keys()),  # number of vocab for age embedding\n",
    "    'max_position_embedding': global_params['max_len_seq'],  # maximum number of tokens\n",
    "    'hidden_dropout_prob': 0.3,  # dropout rate\n",
    "    'num_hidden_layers': 4,  # number of multi-head attention layers required\n",
    "    'num_attention_heads': 6,  # number of attention heads\n",
    "    'attention_probs_dropout_prob': 0.4,  # multi-head attention dropout rate\n",
    "    'intermediate_size': 108,  # the size of the \"intermediate\" layer in the transformer encoder\n",
    "    'hidden_act': 'gelu',\n",
    "    'initializer_range': 0.02,  # parameter weight initializer range\n",
    "    'num_treatment': global_params['treatments'],\n",
    "    'device': global_params['device'],\n",
    "    'year_vocab_size': len(YearVocab['token2idx'].keys()),\n",
    "\n",
    "    'batch_size': global_params['batch_size'],\n",
    "    'MEM': True,\n",
    "    'poolingSize': 50,\n",
    "    'unsupVAE': True,\n",
    "    'unsupSize': ([[3,2]] *22) ,\n",
    "    'vaelatentdim': 40,\n",
    "    'vaehidden': 50,\n",
    "    'vaeinchannels':39,\n",
    "\n",
    "\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin experiments....\n",
      "_________________\n",
      "fold___0\n",
      "_________________\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot access accelerator device when none is available.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m model_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mklpar\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;28mlen\u001b[39m(datatrain)\u001b[38;5;241m/\u001b[39mglobal_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     36\u001b[0m conf \u001b[38;5;241m=\u001b[39m BertConfig(model_config)\n\u001b[1;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m TBEHRT(conf, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# 配置优化器\u001b[39;00m\n\u001b[0;32m     40\u001b[0m optim \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39madam(params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mnamed_parameters()), config\u001b[38;5;241m=\u001b[39moptim_config)\n",
      "File \u001b[1;32mC:\\Project/HealthDataInst/T-BEHRT/Targeted-BEHRT\\src\\model.py:174\u001b[0m, in \u001b[0;36mTBEHRT.__init__\u001b[1;34m(self, config, num_labels)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39motherDevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert \u001b[38;5;241m=\u001b[39m SimpleBEHRT (config)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# self.bert = BertModel(config)\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtreatmentC \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mpoolingSize, config\u001b[38;5;241m.\u001b[39mnum_treatment)\n",
      "File \u001b[1;32mC:\\Project/HealthDataInst/T-BEHRT/Targeted-BEHRT\\src\\model.py:444\u001b[0m, in \u001b[0;36mSimpleBEHRT.__init__\u001b[1;34m(self, config, num_labels)\u001b[0m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39motherDevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m--> 444\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    446\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_labels \u001b[38;5;241m=\u001b[39m num_labels\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39mhidden_dropout_prob)\n",
      "File \u001b[1;32mc:\\Programming\\miniconda3\\envs\\TBEHRT\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1302\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move and/or cast the parameters and buffers.\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m \n\u001b[0;32m   1219\u001b[0m \u001b[38;5;124;03m    This can be called as\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1300\u001b[0m \n\u001b[0;32m   1301\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1302\u001b[0m     device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\n\u001b[0;32m   1303\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1304\u001b[0m     )\n\u001b[0;32m   1306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (dtype\u001b[38;5;241m.\u001b[39mis_floating_point \u001b[38;5;129;01mor\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mis_complex):\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot access accelerator device when none is available."
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "data = pd.read_parquet (file_config['data'])\n",
    "\n",
    "# 创建5折交叉验证对象\n",
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 2)\n",
    "\n",
    "print('Begin experiments....')\n",
    "\n",
    "# 进行5次交叉验证实验\n",
    "for cutiter in (range(5)):\n",
    "    print(\"_________________\\nfold___\" + str(cutiter) + \"\\n_________________\")\n",
    "    data = pd.read_parquet (file_config['data'])\n",
    "\n",
    "    # 获取当前折的训练集和测试集索引\n",
    "    result = next(kf.split(data), None)\n",
    "\n",
    "    # 根据索引分割数据集\n",
    "    # result[0]包含了当前折的训练集索引\n",
    "    # data.iloc[result[0]]使用这些索引从原始数据中选择训练样本\n",
    "    # reset_index(drop=True)重置索引,确保训练集的索引从0开始连续\n",
    "    # 最终得到的datatrain就是当前折的训练数据集\n",
    "    datatrain = data.iloc[result[0]].reset_index(drop=True)\n",
    "    testdata =  data.iloc[result[1]].reset_index(drop=True)\n",
    "\n",
    "    # 构建测试数据集\n",
    "    tset = TBEHRT_data_formation(BertVocab['token2idx'], testdata, code= 'code', \n",
    "                                 age = 'age', year = 'year' , static= 'static' , \n",
    "                                 max_len=global_params['max_len_seq'],expColumn='explabel', outcomeColumn='label',  \n",
    "                                 yvocab=YearVocab['token2idx'], list2avoid=None, MEM=False)\n",
    "    \n",
    "    # 创建测试数据加载器    \n",
    "    testload = DataLoader(dataset=tset, batch_size=int(global_params['batch_size']), shuffle=False, num_workers=0)\n",
    "\n",
    "    # 配置模型参数\n",
    "    model_config['klpar']= float(1.0/(len(datatrain)/global_params['batch_size']))\n",
    "    conf = BertConfig(model_config)\n",
    "    model = TBEHRT(conf, 1)\n",
    "\n",
    "    # 配置优化器\n",
    "    optim = optimizer.adam(params=list(model.named_parameters()), config=optim_config)\n",
    "\n",
    "    # 设置模型保存路径\n",
    "    model_to_save_name =  'TBEHRT_Test' + \"__CUT\" + str(cutiter) + \".bin\"\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(action='ignore')\n",
    "\n",
    "    # 设置学习率调度器\n",
    "    scheduler = toptimizer.lr_scheduler.ExponentialLR(optim, 0.95, last_epoch=-1)\n",
    "    patience = 0\n",
    "    best_pre = -100000000000000000000\n",
    "    LossC = 0.1\n",
    "\n",
    "    # 无监督预训练\n",
    "    for e in range(2):\n",
    "        scheduler , patience= trainunsup(e, scheduler, patience)\n",
    "\n",
    "    # 有监督训练\n",
    "    for e in range(2):\n",
    "        train_multi(e)\n",
    "        # 评估模型性能\n",
    "        auc, auroc, auc2, loss = evaluation_multi_repeats()\n",
    "        aucreal = -1 * loss\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if aucreal > best_pre:\n",
    "            patience = 0\n",
    "            print(\"** ** * Saving best fine - tuned model ** ** * \")\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            output_model_file = os.path.join(global_params['output_dir'], model_to_save_name)\n",
    "            create_folder(global_params['output_dir'])\n",
    "            if global_params['save_model']:\n",
    "                torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "            best_pre = aucreal\n",
    "            print(\"auc-mean: \", aucreal)\n",
    "        else:\n",
    "            # 学习率衰减\n",
    "            if patience % 2 == 0 and patience != 0:\n",
    "                scheduler.step()\n",
    "                print(\"LR: \", scheduler.get_lr())\n",
    "\n",
    "            patience = patience + 1\n",
    "        print('auprc : {}, auroc : {}, Treat-auc : {}, time: {}'.format(auc, auroc, auc2, \"long.....\"))\n",
    "\n",
    "    # 最终评估\n",
    "    LossC = 0.1\n",
    "    conf = BertConfig(model_config)\n",
    "    model = TBEHRT(conf, 1)\n",
    "    optim = optimizer.VAEadam(params=list(model.named_parameters()), config=optim_config)\n",
    "    output_model_file = os.path.join(global_params['output_dir'], model_to_save_name)\n",
    "    model = toLoad(model, output_model_file)\n",
    "\n",
    "    # 进行全面评估并获取结果\n",
    "    y, y_label, t_label, treatO, tprc, eps = fullEval_4analysis_multi(False, True, testdata)\n",
    "    y2, y2label, treatmentlabel2, treatment2 = fullCONV(y, y_label, t_label, treatO)\n",
    "\n",
    "    # 保存评估结果\n",
    "    NPSaveNAME =  'TBEHRT_Test' + \"__CUT\" + str(cutiter) + \".npz\"\n",
    "    np.savez(  NPSaveNAME,\n",
    "             outcome=y2,\n",
    "             outcome_label=y2label, treatment=treatment2, treatment_label=treatmentlabel2,\n",
    "             epsilon=np.array([0]))\n",
    "             \n",
    "    # 清理内存\n",
    "    del y, y_label, t_label, treatO, tprc, eps, y2, y2label, treatmentlabel2, treatment2, datatrain, conf, model, optim, output_model_file,  best_pre, LossC,\n",
    "    print(\"\\n\\n\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBEHRT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
